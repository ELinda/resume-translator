{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "22d13b95-1b67-4459-8160-cfd884aae358",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('resume.json') as f:\n",
    "    resume=json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "59ef762a-830f-4827-88df-5f7f42561706",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<table style='width:100%'><tr><td><h1 style='font-size:50'>Linda Cai</h1>Software Engineer</td>\n",
      "<td style='vertical-align:bottom'>linda.cai@zoho.com<br/>https://www.linkedin.com/in/lindagcai/</td></tr></table>\n",
      "<br/>\n",
      "<table style='float:left;width:100%'><tr>\n",
      "                   <td><a href=\"https://www.linkedin.com/company/cognitops/\"><h4>CognitOps</h4></a></td>\n",
      "                   <td>2021-06-30 &#8211; 2022-04-30</td></tr>\n",
      "                   <tr><td colspan=2>Backend Scala engineer for warehouse management tool featuring analytics and reporting.<br/>&nbsp; &nbsp;&#8226;&nbsp; Updated Scala code in Kafka consumers/producers, reporting transformations, etc.<br/>&nbsp; &nbsp;&#8226;&nbsp; Maintained API endpoints<br/>&nbsp; &nbsp;&#8226;&nbsp; Updated data models to add features or simplify logic<br/>&nbsp; &nbsp;&#8226;&nbsp; Gathered requirements and created design documents<br/>&nbsp; &nbsp;&#8226;&nbsp; Added features to custom SBT plugin<br/>Used Google Cloud Platform (GCP) to monitor Kubernetes deployments<br/>Managed configurations (in JavaScript) for customizing user interface per customer<br/>Introduced a small amount of infrastructure-as-code in Terraform (for backups), and modified Terraform files (for micro-service architecture).<br/>Assisted in customer integrations, involving complex queries on a variety of databases (Oracle, Microsoft SQL Server, etc). Constructed some queries and reviewed queries written by other developers and consultants, for performance and adherence to data model.</td></tr></table><br/><table style='float:left;width:100%'><tr>\n",
      "                   <td><a href=\"https://www.linkedin.com/company/capital-one/\"><h4>Capital One</h4></a></td>\n",
      "                   <td>2019-04-30 &#8211; 2021-06-30</td></tr>\n",
      "                   <tr><td colspan=2>Work on various data cataloging projects, and a streaming data transformation platform.<br/>Reduced repetition by creating reusable modules/dependencies (or reusing existing ones where possible).<br/>Optimized Scala Spark and PySpark applications, including regular expressions, for large scale data.<br/>Automated deployments and management of MapReduce clusters, using Jenkins, Terraform, and the AWS SDK.<br/>Created restrictive IAM and bucket policies. Integrated with Vault service and ensured secrets were kept there.<br/>Created cloud event driven monitoring function which integrated with Slack.<br/>Forwarded logs to be easily searched in Splunk.<br/>Technologies used: Scala, Python, Groovy, Java, Terraform, Spark, Docker, Akka, Datadog, New Relic.<br/>AWS services used: EMR, Lambda, Cloudwatch</td></tr></table><br/><table style='float:left;width:100%'><tr>\n",
      "                   <td><a href=\"https://www.linkedin.com/company/the-walt-disney-studios/\"><h4>The Walt Disney Studios</h4></a></td>\n",
      "                   <td>2017-10-31 &#8211; 2018-11-30</td></tr>\n",
      "                   <tr><td colspan=2>Maintained and updated streaming data application for tracking theatre package info for set of Tableau reports<br/>  &nbsp; &nbsp;&#8226;&nbsp;  Used Scala, Java, OpenShift, Postgres, Flyway, Confluent Kafka, Kafka Connect, KStreams<br/>  &nbsp; &nbsp;&#8226;&nbsp;  Added monitoring and alerting for all five components, and performed full documentation/knowledge transfer<br/>  &nbsp; &nbsp;&#8226;&nbsp;. Contributed to Avro schema, Tableau report extractor, and KStreams changes, working with Theatrical team<br/>  &nbsp; &nbsp;&#8226;&nbsp;  Investigated and documented issues with ingestion from RabbitMQ, such as connection failures and missing data<br/>  &nbsp; &nbsp;&#8226;&nbsp;  Reviewed all major queries, optimizing as necessary<br/><br/>Conducted Dataiku and Databricks POCs, and helped other teams migrate Python, SQL, and R workflows to DataBricks<br/>  &nbsp; &nbsp;&#8226;&nbsp;  Set up Dataiku testing endpoint for internal Disney Music Group users, investigated most features, and provided a demo<br/>  &nbsp; &nbsp;&#8226;&nbsp;  Attended and served in training and on-boarding sessions, and served as point of contact for internal DataBricks users in other groups (data scientists, analysts, and engineers)<br/>  &nbsp; &nbsp;&#8226;&nbsp;  Created and ran presentations to encourage adoption<br/>  &nbsp; &nbsp;&#8226;&nbsp;  Provided ongoing support for users, troubleshooting specific data and notebook issues, writing Python notebooks to provide workarounds for some issues<br/>  &nbsp; &nbsp;&#8226;&nbsp;  Administered notebook clusters (users, cluster resources), managed libraries (public PyPi and custom Eggs)<br/>  &nbsp; &nbsp;&#8226;&nbsp;  Documented processes for users: backup for notebooks, data upload and access<br/><br/>With team, designed, investigated, and implemented new data platform components<br/>  &nbsp; &nbsp;&#8226;&nbsp;  Jenkins scripts using OpenShift commands to allow automatic build pipeline creation and trigger<br/>  &nbsp; &nbsp;&#8226;&nbsp;  Reviewed and implemented some AirFlow workflow scheduler plugins, configuration, and deployment<br/><br/>Managed infrastructure, using OpenShift, a Kubernetes-based containerization platform<br/>  &nbsp; &nbsp;&#8226;&nbsp; Edited OpenShift object files, such as build configs, cron jobs, resource limits, and image streams<br/>  &nbsp; &nbsp;&#8226;&nbsp; Investigated issues such as jobs being killed or not scheduling</td></tr></table><br/><table style='float:left;width:100%'><tr>\n",
      "                   <td><a href=\"https://www.linkedin.com/company/jwplayer/\"><h4>JW Player</h4></a></td>\n",
      "                   <td>2014-10-31 &#8211; 2017-07-31</td></tr>\n",
      "                   <tr><td colspan=2>&nbsp; &nbsp;&#8226;&nbsp; With two other team members, created the first data pipeline used for video recommendations, using Luigi, Numpy, Gensim Python libraries, among others<br/>&nbsp; &nbsp;&#8226;&nbsp; Optimized storm topology sink (real-time output) performance, reducing space utilization in real-time pipeline's target Mongo DB by 5X<br/>&nbsp; &nbsp;&#8226;&nbsp; Added new functionality into storm query engine code base, e.g. updating sets in Redis, and adding new topologies (real-time pipelines) for recommendations and publisher reports.<br/>&nbsp; &nbsp;&#8226;&nbsp; Maintained existing batch pipelines<br/>&nbsp; &nbsp;&#8226;&nbsp; &nbsp; &nbsp;&#8226;&nbsp; Updated versions of Hadoop and other libraries for compatibility<br/>&nbsp; &nbsp;&#8226;&nbsp; &nbsp; &nbsp;&#8226;&nbsp; Added monitoring and alerting using Nagios at first, and then Datadog (using statsd-like features)<br/>&nbsp; &nbsp;&#8226;&nbsp; &nbsp; &nbsp;&#8226;&nbsp; Added library functions for common tasks like launching a cluster or monitoring pipelines<br/>&nbsp; &nbsp;&#8226;&nbsp; &nbsp; &nbsp;&#8226;&nbsp; Made configuration and infrastructure changes to save money or handle larger data volume<br/>&nbsp; &nbsp;&#8226;&nbsp; Performed support/investigation of various issues, e.g. inconsistent or missing values, data population stalling or lagging behind, etc.<br/>&nbsp; &nbsp;&#8226;&nbsp; &nbsp; &nbsp;&#8226;&nbsp; Where needed, worked on fall-back or self-healing features (e.g. auto-restart)<br/>&nbsp; &nbsp;&#8226;&nbsp; Conducted various technical interviews, added interview questions, and trained other interviewers; helped with onboarding<br/>&nbsp; &nbsp;&#8226;&nbsp; Investigated new technologies and algorithms (e.g. approximation and statistical), attended conferences/talks, and managed five interns from Saxion University with another team member.<br/>&nbsp; &nbsp;&#8226;&nbsp; &nbsp; &nbsp;&#8226;&nbsp; Member of a deep learning club for neural nets, investigated using Cassandra, Dynamo DB, Redshift, and Kinesis<br/>&nbsp; &nbsp;&#8226;&nbsp; Created documentation, added unit and integration testing (for own functions and existing ones that lacked testing), and performed code reviews for over a dozen projects (repositories on GitHub).</td></tr></table><br/><table style='float:left;width:100%'><tr>\n",
      "                   <td><a href=\"https://www.linkedin.com/company/appnexus/\"><h4>AppNexus</h4></a></td>\n",
      "                   <td>2013-03-31 &#8211; 2014-06-30</td></tr>\n",
      "                   <tr><td colspan=2>I worked in the Data Team on data pipeline (largely Hadoop-based) processing and validation. Using a combination of Hive, Java, Vertica, PHP, Unix/Linux/HDFS commands, and MySQL, I<br/>&nbsp; &nbsp;&#8226;&nbsp; Worked on a Java web service that provides safer and more automated validation; configured and maintained validation jobs. <br/>&nbsp; &nbsp;&#8226;&nbsp; Worked as tech lead with UI, API, and Product teams to provide mobile app reporting to clients. <br/>&nbsp; &nbsp;&#8226;&nbsp; Combined billing aggregations to streamline the end-of-month invoice process for the billing team. <br/>&nbsp; &nbsp;&#8226;&nbsp; Migrated part of the pipeline to use protobuf, to save HDFS space. <br/>&nbsp; &nbsp;&#8226;&nbsp; Discovered, investigated, and worked with relevant owners to fix various bugs throughout the system. <br/>&nbsp; &nbsp;&#8226;&nbsp; Worked on entire development process, including requirements discussion, coding, reviewing, testing, deployment, job scheduling, and support. Much support was to troubleshoot MapReduce and PHP jobs with JobTracker interface and logs.</td></tr></table><br/><table style='float:left;width:100%'><tr>\n",
      "                   <td><a href=\"https://www.linkedin.com/company/citi/\"><h4>Citigroup</h4></a></td>\n",
      "                   <td>2010-07-31 &#8211; 2013-02-28</td></tr>\n",
      "                   <tr><td colspan=2>(Continued to this full-time position from 2009 internship). <br/>I worked in the regulatory reporting group for order router and ECN products, within Citi Equities Technologies, to develop and maintain reports, data-related business processes, and a GUI application.<br/>I was responsible for development of the transactional billing system, reference data maintenance user interface, and various SEC/FINRA-mandated compliance reports. Per project, I coordinated requirements, development, and testing with ﬁnance, compliance, and other technology staff. <br/>Also, as needed, to accommodate DBMS upgrades and hardware changes, I worked on system migrations, learning new systems and automating parallel testing.</td></tr></table>\n"
     ]
    }
   ],
   "source": [
    "bull = '&#8226;'\n",
    "profiles = bull.join(prof['url'] for prof in resume['basics']['profiles'])\n",
    "\n",
    "dash = '&#8211;'\n",
    "def transform_work_entry(entry):\n",
    "    summary = entry['summary'].replace('\\n','<br/>').replace('*', '&nbsp; &nbsp;' + bull + '&nbsp;')\n",
    "    transformed = f\"\"\"<table style='float:left;width:100%'><tr>\n",
    "                   <td><a href=\"{entry['url']}\"><h4>{entry['name']}</h4></a></td>\n",
    "                   <td>{entry['startDate']} {dash} {entry['endDate']}</td></tr>\n",
    "                   <tr><td colspan=2>{summary}</td></tr></table>\"\"\"\n",
    "    return transformed\n",
    "\n",
    "experience = '<br/>'.join(transform_work_entry(entry) for entry in resume['work'])\n",
    "\n",
    "header = f\"\"\"<table style='width:100%'><tr><td><h1 style='font-size:50'>{resume['basics']['name']}</h1>{resume['basics']['label']}</td>\n",
    "<td style='vertical-align:bottom'>{resume['basics']['email']}<br/>{profiles}</td></tr></table>\"\"\"\n",
    "print(header + '\\n<br/>\\n' + experience)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
